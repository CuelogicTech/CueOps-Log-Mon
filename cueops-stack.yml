version: "3.3"

networks:
  cueops:
    driver: overlay
    attachable: true

volumes:
    prometheus: {}
    grafana: {}
    alertmanager: {}
    esdata: {}
    elastalert_logs: {}

configs:
  caddy_config:
    file: ./caddy/Caddyfile
  dockerd_config:
    file: ./dockerd-exporter/Caddyfile
  node_rules:
    file: ./prometheus/rules/swarm_node.rules.yml
  task_rules:
    file: ./prometheus/rules/swarm_task.rules.yml
  logstash_cofig:
    file: ./logstash/logstash.conf

services:
  dockerd-exporter:
    image: stefanprodan/caddy
    networks:
      - cueops
    environment:
      - DOCKER_GWBRIDGE_IP=172.18.0.1
    configs:
      - source: dockerd_config
        target: /etc/caddy/Caddyfile
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  cadvisor:
    image: google/cadvisor
    networks:
      - cueops
    command: -logtostderr -docker_only
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  grafana:
    image: stefanprodan/swarmprom-grafana:4.6.3
    networks:
      - cueops
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-localhost}
      #- GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}
      #- GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@test.com}
      #- GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana}
      #- GF_SMTP_HOST=${GF_SMTP_HOST:-smtp:25}
      #- GF_SMTP_USER=${GF_SMTP_USER}
      #- GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD}
    volumes:
      - grafana:/var/lib/grafana
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  alertmanager:
#    image: stefanprodan/swarmprom-alertmanager:v0.14.0
    image: cueops/alertmanager:v1.0
    networks:
      - cueops
    environment:
      - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}
      - SLACK_CHANNEL=${SLACK_CHANNEL:-general}
      - SLACK_USER=${SLACK_USER:-alertmanager}
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - alertmanager:/alertmanager
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  unsee:
    image: cloudflare/unsee:v0.8.0
    networks:
      - cueops
    environment:
      - "ALERTMANAGER_URIS=default:http://alertmanager:9093"
    deploy:
      mode: replicated
      replicas: 1

  node-exporter:
    image: stefanprodan/swarmprom-node-exporter:v0.15.2
    networks:
      - cueops
    environment:
      - NODE_ID={{.Node.ID}}
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.textfile.directory=/etc/node-exporter/'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      # no collectors are explicitely enabled here, because the defaults are just fine,
      # see https://github.com/prometheus/node_exporter
      # disable ipvs collector because it barfs the node-exporter logs full with errors on my centos 7 vm's
      - '--no-collector.ipvs'
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  prometheus:
#    image: stefanprodan/swarmprom-prometheus:v2.2.0-rc.0
    image: cueops/prometheus:v1.0
    networks:
      - cueops
    environment:
#      - PROM_USERNAME=${PROM_USERNAME:-admin}
#      - PROM_PASSWORD=${PROM_PASSWORD:-admin}
      - PROM_IP=${PROM_IP:-localhost}
      - PROM_PORT=${PROM_PORT:-9090}
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention=24h'
    volumes:
      - prometheus:/prometheus
    configs:
      - source: node_rules
        target: /etc/prometheus/swarm_node.rules.yml
      - source: task_rules
        target: /etc/prometheus/swarm_task.rules.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 500M
        reservations:
          memory: 250M

  caddy:
    image: stefanprodan/caddy
    ports:
      - "3000:3000"
      - "9090:9090"
      - "9093:9093"
      - "9094:9094"
      - "5601:5601"
    networks:
      - cueops
    environment:
      - ADMIN_USER=${ADMIN_USER:-admin}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
    configs:
      - source: caddy_config
        target: /etc/caddy/Caddyfile
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:3000"]
      interval: 5s
      timeout: 1s
      retries: 5

################################################################
#                      Logging Stack                           #
################################################################                      

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:5.3.2
    networks:
      - cueops
    environment:
      ES_JAVA_OPTS: '-Xms256m -Xmx256m'
      xpack.security.enabled: 'false'
      xpack.monitoring.enabled: 'false'
      xpack.graph.enabled: 'false'
      xpack.watcher.enabled: 'false'
    volumes:
      - esdata:/usr/share/elasticsearch/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager

  logstash:
    image: docker.elastic.co/logstash/logstash:5.3.2
    networks:
      - cueops
    ports:
      - "5000:5000" 
    configs:
      - source: logstash_cofig
        target: /usr/share/logstash/pipeline/logstash.conf
    #volumes:
    #  - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch
    deploy:
      replicas: 1

  logspout:
    image: cueops/logspout-logstash:v1.0
    networks:
      - cueops
    environment:
      ROUTE_URIS: 'logstash://logstash:5000'
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - logstash
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
        delay: 30s

  kibana:
    image: docker.elastic.co/kibana/kibana:5.3.2
    networks:
      - cueops
#    ports:
#      - '5601:5601'
    depends_on:
      - elasticsearch
    environment:
      ELASTICSEARCH_URL: 'http://elasticsearch:9200'
      XPACK_SECURITY_ENABLED: 'false'
      XPACK_MONITORING_ENABLED: 'false'
    deploy:
      replicas: 1

  elastalert:
    #image: ivankrizsan/elastalert:0.1.8
    image: cueops/elastalert:v1.1
    ports:
        - "3030:3030"
    networks:
        - cueops
    depends_on:
        - elasticsearch
    environment:
        - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}
        - SLACK_CHANNEL=${SLACK_CHANNEL:-general}
        - SLACK_USER=${SLACK_USER:-alertmanager}
        - ELASTALERT_CONFIG=elastalertconfig.yaml
        - CONFIG_DIR=/opt/config
        - LOG_DIR=/opt/logs
        - ELASTALERT_CONFIG=/opt/config/elastalertconfig.yaml
        - ELASTICSEARCH_PORT=9200
        - ELASTICSEARCH_HOST=elasticsearch
        - ELASTALERT_SUPERVISOR_CONF=/opt/config/elastalert_supervisord.conf
    volumes:
        #- /var/dockerdata/elastalert/config:/opt/config
        #- /var/dockerdata/elastalert/rules:/opt/rules
        - elastalert_logs:/opt/logs
    deploy:
        mode: replicated
        replicas: 1
        update_config:
            parallelism: 1
            delay: 60s
        restart_policy:
            condition: on-failure
            max_attempts: 5
